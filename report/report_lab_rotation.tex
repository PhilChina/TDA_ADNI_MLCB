\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{}{natbib}
% before loading neurips_2020

% ready for submission
%\usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%\usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{neurips_2020}
% to avoid loading the natbib package, add option nonatbib:
\usepackage{neurips_2020}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{amsmath}
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{url}
\usepackage{textgreek}
\usepackage{amsfonts}
\usepackage{caption, subcaption}
\usepackage[pdftex]{graphicx}
\usepackage{tikz}
\usepackage{xcolor}

\usetikzlibrary{shapes,arrows}
\tikzstyle{orange} = [rectangle, draw, fill=YellowOrange!30,
    text width=7em, text centered, rounded corners]
\tikzstyle{orange} = [rectangle, draw, fill=YellowOrange!30,
    text width=7em, text centered, rounded corners]
\tikzstyle{orange} = [rectangle, draw, fill=YellowOrange!30,
    text width=7em, text centered, rounded corners]

\tikzset{
  myarrow/.style={->, >=latex', shorten >=1pt, thick},
	}

\hypersetup{
  colorlinks   = true,
  linkcolor = blue,
  anchorcolor = blue,
  citecolor = black,
  % filecolor,
  % menucolor,
  % runcolor,
  urlcolor = cyan,
  % allcolors = black,
}

\title{Uncovering the topology of the medial temporal lobe in Alzheimer's disease.}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{Philip Hartout\thanks{With thanks to Bastian Rieck for the supervision and Sarah
  Brueningk, Felix Hensel, Catherine Jutzeler, Merel Kuijs, and Louis Lukas for insightful discussions, code, and data. Last compiled: \today}\\ Department of Biosystems Science and Engineering\\ ETH Zürich\\ Zürich, Switzerland \\ \texttt{phartout@ethz.ch} \\ } \date{\today}

\begin{document}

\maketitle

\begin{abstract}
Topological data analysis on medical imaging data is an emerging field
leveraging the shape of data for various application domains, including machine
learning tasks. Here, we apply a topological data analysis pipeline to uncover
novel insights in the Alzheimer's disease neuroimaging initiative dataset by
applying persistent homology on a patch of the medial temporal lobe to extract
persistence images and persistence landscapes. We use these representations to
(i) learn to classify patients suffering from Alzheimer's disease from
cognitively normal patients thereby assessing the saliency of persistence images
extracted from the most affected brain regions in Alzheimer's disease, (ii)
analyze the topological heterogeneity of each of the diagnostic categories, and
(iii) across various time points available for each patient. Finally, we use
these representations to (iv) attempt to cluster the patients according to their
diagnosis and disease subtypes, when applicable.
\end{abstract}

A graphical abstract of the analyses discussed in this report is shown in
Figure~\ref{fig:graphical_abstract}

\begin{figure}[b]
  \centering
  \begin{tikzpicture}[align=center, node distance = 1cm, auto]
    \node[](unprocessed) {\includegraphics[width=0.15\textwidth]{figures/t1_unprocessed.jpg}};
    \node[below of=unprocessed, yshift=-0.3cm](unprocessed_txt){Unprocessed};

    \node[right of=unprocessed, xshift=2.5cm](processed)
         {\includegraphics[width=0.15\textwidth]{figures/T1_BET.png}}; \node[below of=processed,
           yshift=-0.3cm](processed_txt) {\textit{fMRIPrep}};
         \draw[myarrow](unprocessed.east)--(processed.west);

    \node[right of=processed, xshift=2.5cm](selection)
         {\includegraphics[width=0.15\textwidth]{figures/patch_performance.png}}; \node[below
           of=selection, yshift=-0.3cm](selection_txt) {Patch selection};
         \draw[myarrow](processed.east)--(selection.west);

    \node[right of=selection, xshift=2.5cm](persistence_homology)
         {\includegraphics[width=0.15\textwidth]{figures/PDs/persistence_diagram_CN.png}};
         \node[below of=persistence_homology, yshift=-0.3cm](persistence_homology_txt) {Persistence
           homology}; \draw[myarrow](selection.east)--(persistence_homology.west);

    \node[below of=selection_txt, yshift=-1.3cm](persistence_image)
         {\includegraphics[width=0.15\textwidth]{figures/PIs/Persistence_image_CN_h_1.png}};
         \node[below of=persistence_image, yshift=-0.3cm](persistence_image_txt) {Persistence
           image}; \draw[myarrow](persistence_homology_txt.south)--(persistence_image.north);

    \node[left of=persistence_image, xshift=-2.5cm](classification)
         {\includegraphics[width=0.15\textwidth]{figures/neuralnetwork.png}}; \node[below
           of=classification, yshift=-0.3cm](classification_txt) {Classification};
         \draw[myarrow](persistence_image.west)--(classification.east);

    \node[left of=classification, xshift=-2.5cm](misclassified)
         {\includegraphics[width=0.15\textwidth]{figures/misclassified.png}}; \node[below
           of=misclassified, yshift=-0.3cm](misclassified_txt) {Misclassified samples\\ analysis};
         \draw[myarrow](classification.west)--(misclassified.east);

    \node[right of=persistence_image, xshift=2.5cm](persistence_landscape)
         {\includegraphics[width=0.15\textwidth]{figures/Persistence_landscape_CN.png}}; \node[below
           of=persistence_landscape, yshift=-0.3cm](persistence_landscape_txt) {Persistence
           landscape};
         \draw[myarrow](persistence_homology_txt.south)--(persistence_landscape.north);

    \node[below of=persistence_landscape_txt, yshift=-1.6cm](median_persistence_landscape)
         {\includegraphics[width=0.15\textwidth]{figures/median_pls/median_pl_CN.png}\\ \includegraphics[width=0.15\textwidth]{figures/median_pls/median_pi_CN_h_0.png}};
         \node[below of=median_persistence_landscape,
           yshift=-1.3cm](median_persistence_landscape_txt) {Median persistence \\landscape and
           image};
         \draw[myarrow](persistence_landscape_txt.south)--(median_persistence_landscape.north);
         \draw[myarrow](persistence_image_txt.south)--(median_persistence_landscape.north);

    \node[left of=median_persistence_landscape, xshift=-2.5cm](distance_analysis)
         {\includegraphics[width=0.15\textwidth]{figures/PIs/displot_median_pi_CN_H_0.png}};
         \node[below of=distance_analysis, yshift=-0.3cm](distance_analysis_txt) {Distance
           analysis}; \draw[myarrow](median_persistence_landscape.west)--(distance_analysis.east);
         %\draw[myarrow](misclassified_txt.south)--(distance_analysis.north);

    \node[left of=distance_analysis, xshift=-2.5cm](clustering)
         {\includegraphics[width=0.15\textwidth]{figures/clustering.png}}; \node[below
           of=clustering, yshift=-0.3cm](clustering_txt) {Visualisation of distance \\to two median
           images}; \draw[myarrow](distance_analysis.west)--(clustering.east);

  \end{tikzpicture}
  \caption{Flow chart of the analyses conducted in this report. Images adapted from
    \href{https://commons.wikimedia.org/wiki/Brain/media/File:MRI_head_side.jpg}{Wikimedia},
    \href{https://www.slicer.org/wiki/Documentation/Nightly/Modules/BrainVolumeRefinement}{
      slicer.org}, \href{https://thenounproject.com/xela./collection/diagrams/?i=486221}{Xela Ub}
    and \href{https://thenounproject.com/smodgekar/collection/data-classify/}{Sachin Modgekar}}
\label{fig:graphical_abstract}
\end{figure}

\section{Introduction}

\subsection{Alzheimer's disease}\label{sec:ad_context}

Alzheimer's disease (AD) is the most prevalent form of dementia in the world, with a forecasted 75
million cases in 2030 and 132 million by 2050 \citep{world2017global}. In EU member states and
Switzerland, AD is already among the leading causes of death and is projected to further accelerate
in the future \citep{sleeman2019escalating}. The associated costs are immense --- in the United
States alone, the cost of care of AD patients is expected to be \$2 trillion by 2030 ---, and are
poised to substantially burden the economic prosperity of developed countries in the future
~\citep{world2017global}. Although the definite diagnosis of a patient with AD can only be done
post-mortem, clinicians use a plethora of standardized tools to find indications of the developing
pathology as early as possible, ranging from neuropsychological tests, blood and cerebrospinal fluid
biomarkers, to MRI images ~\citep{mckhann2011diagnosis, smits2012early, lehmann2016biomarkers}.

Although there are disputes on the root cause of the disease in the late-onset form of AD
\citep{tharp2013origins, fulop2018can, hur2020innate}, there is a wide consensus that the presence
of Amyloid \textbeta{} (A\textbeta{}), originating from cleavage of the amyloid precursor protein
(APP), together with the aggregation of neurofibrillary tangles, stemming from hyperphosphorylated
tau proteins, accumulate in the brain of patients with AD and leads to neural cell death
~\citep{da2016insights}. This cellular destruction leads to a cumulative effect: brain atrophy,
which refers to the shrinkage of brain volume. This damage particularly affects brain regions
involved in memory formation such as the medial temporal lobe (MTL), which contains the entorhinal
cortex, the hippocampus, and the amygdala \citep{goedert2006century} is visible
using structural Magnetic Resonance Imaging (sMRI) ~\citep{frisoni2010clinical}, and has been
heralded as one of the most reliable biomarkers for Alzheimer's disease \citep{pini2016brain}.

Such images provide a rich source of data, which can then be used for various purposes. One of them
is classification, which consists of categorizing cognitively intact subjects (CN) and AD patients
using deep learning techniques such as convolutional neural networks ~\citep{wen2020convolutional}.
Additionally, sMRI data can be used to identify multiple regions affected by the disease, and
patterns have emerged as to which groups of regions are affected, leading to the definition of
various subtypes of AD ~\citep{poulakis2018heterogeneous,tijms2020pathophysiological}. To gain even
finer insights from the observable alterations of brain shape in the context of AD with minimal
computation, ideas stemming from the mathematical field of topology can be applied. Topology
concerns itself with properties of geometric objects under continuous deformations, such as
stretching, twisting, crumpling, and bending. Such deformations aptly summarize the type of
deformation occurring to the brain due to AD and therefore makes an ideal context in which we can
apply computational topology to uncover and quantify anatomical changes resulting from the disease.

\subsection{Topology}

Topology has witnessed relentless theoretical progress since Henri Poincaré first addressed
topological ideas as a distinct branch of mathematics in his 1895 publication of \textit{Analysis
  Situs}~\citep{poincare1895analysis, james1999history}. Only recently, -- with the advent of
modern computing -- has the field of computational topology and topological data analysis (TDA) gained
momentum to investigate (high-dimensional) data in physics, biology, and
beyond~\citep{dey1999computational, ghrist2008barcodes, amezquita2020shape}. While surveying the
various applications of computational topology are beyond the scope of this report, we still want to
define several procedures that are paramount to the workflow described in this report: cubical
persistence, various vectorized representations of the persistence diagrams obtained from filtered
cubical complexes, and the notion of pairwise distance between such representations. For material
providing an extensive and formal introduction to topology and persistent homology, please refer to
~\citep{freedman2009algebraic, edelsbrunner2010computational, ghrist2008barcodes}.


\subsubsection{Cubical complexes and persistent homology}

Before defining the notion of cubical persistence, \emph{cubical complexes} need to be defined.
For that, let us first assign to each elementary non-degenerate interval $[a,a+1]\forall
a\in\mathbb{R}$, two degenerate intervals $[a,a]$ and $[a+1,a+1]$. For a $d$-dimensional space, a
cube is then defined as a product of $d$ elementary intervals $\Pi_{i=1}^{d}I_i$. The dimension of
the cube is then equal to the number of non-degenerate interval in the aforementioned product, such
that 0-cubes, 1-cubes, 2-cubes, and 3-cubes correspond to vertices, edges, squares, and 3D cubes,
respectively. In this report, the 3D cube corresponds to a given voxel of our sMRI.

A cubical complex $X$ of dimension $d$ is then defined as a finite set of elementary cubes of at
most dimension $d$, where $X$ must be closed undertaking faces and intersections, i.e. for any cube
in $X$, all of its faces must belong to $X$, and for any two cubes in $X$, their intersection is
either empty, or there is a common face between them.

Given a topological space $\mathbb{X}$ (in our case, the sMRI voxels), we use a filtering function
$f:\mathbb{X}\to\mathbb{R}$, which here corresponds to the pixel density of each voxel, to study the
topology of the sublevel sets $\mathbb{X}_t=f^{-1}(-\infty, t]$ of cubical complexes that arise.
This is a method to study topological spaces referred to as \emph{persistent homology}. A common
representation of the evolution of topological complexes as a function of the value of the
filtration function is the \emph{persistence diagram} (PD), which is a multiset of points. For each
homological dimension (here, $0,1,2$), we obtain a collection of points, with an associated $x$ and
$y$ coordinate which corresponds to the birth and death of a topological feature in the homology
dimension $n=0,1,2,\ldots$, respectively. We refer to a homological feature as \emph{persistent} if
the difference between its birth and death value is
particularly high.


\subsubsection{Persistence images and persistence landscapes}\label{sec:theory_persistence_landscape_persistence_image}

PDs are endowed with a metric space; the $p$-Wasserstein distances can be computed between any two
persistence image, and these metrics are stable under small perturbations of the data
\citep{skraba2020wasserstein}. So, it is possible to perform a variety of machine learning (ML)
techniques using (representations) PDs as input data. However, multiple ML algorithms require more than
a metric, and the computation cost of the $p$-Wasserstein distance increases linearly with the
number of points in the PD. It is therefore desirable to have a fixed-length, stable, efficient-to-compute,
interpretable (with respect to the PD) and tunable mapping from the PD to a vector space in
$\mathbb{R}^n$ to fit various machine learning algorithms to them.

One such representation is the \emph{persistence image} (PI) of a PD, which has been proven to be
stable upon small perturbations of data while still retaining the underlying features in the data
useful for classification. Computing the PI from a PD $D$ consists of
a two-step process. First, the PD is mapped to an integrable function $\rho_D : \mathbb{R}^2\to
\mathbb{R}$ called a persistent surface. This surface is a weighted sum of Gaussian distributions,
each centered around a point of the PD. The matrix of pixel values can be obtained from the
computation of the integration of $\rho_D$ on a grid overlaid on the surface
~\citep{adams2017persistence}.

Another representation associated to the PD is the persistence landscape
(PL). This representation maps the PD into a
Hilbert space, which is useful for ML applications. In order to define a persistence landscape, let
us take a pair $(b,d)$, which refer to the birth and death of a feature. We now define the piecewise
linear function $f_{(b,d)}:\mathbb{R} \to [0, \infty]$ as
\begin{equation}
  \label{eq:piecewise_linear_landscape}
  f_{(b,d)}(x)=
  \begin{cases}
    0 & \text{if }x \notin (b,d)\\ x-b & \text{if }x\in (b,{b+d\over 2}]\\ -x+b & \text{if }x\in
      ({b+d\over 2},d]
  \end{cases}
\end{equation}


The PL of the birth-death pairs $\left\{b_i,d_i\right\}_{i=1}^{n}$ is the sequence of functions
$\lambda_k:\mathbb{R}\to [0,\infty]$, $k=1,2,3,\ldots$ where $\lambda_k(x)$ is the $k^{th}$ largest
value of $\left\{f_{b_i, d_i}(x)\right\}^{n}_{i=1}$. We set $\lambda_k(x)=0$ if the $k^{th}$ largest
value does not exist, which results in $\lambda_k=0$ for $k>n$ ~\citep{bubenik2015statistical, bubenik2020persistence}.
% An example of PL is shown in figure~\ref{fig:pl_bubenik}.

% \begin{figure}
%   \centering
%   \begin{minipage}{60mm}
%     \includegraphics[width=43mm]{figures/persistence_landscape_bubenik_paper/landscapes-figure7.pdf} %     \vspace{1ex}
%     \includegraphics[width=53mm]{figures/persistence_landscape_bubenik_paper/landscapes-figure9.pdf}
%   \end{minipage}
%   \begin{minipage}{60mm}
%     \vspace{5ex}
%     \includegraphics[width=53mm]{figures/persistence_landscape_bubenik_paper/landscapes-figure8.pdf} %     \vspace{-5ex}
%     \includegraphics[width=53mm]{figures/persistence_landscape_bubenik_paper/paper3dlandscape.png}
%   \end{minipage}
%   \vspace{5ex}
%   \caption{PL for the homology in degree 1 of linked annuli from %   \citep{bubenik2015statistical}. Top left, persistence diagram, top %   right, rescaled function from equation %   \ref{eq:piecewise_linear_landscape}. Bottom left and right, 2 and 3d %   representation of the persistent landscape. Note that $\lambda_1$ %   represents the most persistent topological features.}
%   \label{fig:pl_bubenik}
% \end{figure}

\subsubsection{Pairwise distances and medians}

A crucial element in our investigations consists of examining distances between vectorized
topological representations. Intuitively, and as noted by
\citep{berwald2018computing}, it important to take the meaning of the points of the PD into account;
namely that a point close to the diagonal $(c,c+\epsilon)$ represents a feature that lived for a
short time $\epsilon$. A diagram with this small lifetime point should therefore be close to the
same diagram without that point, where the feature would not appear at all. Hence, it makes sense to
introduce the notion of minimal cost required to match up points of the two diagrams, either
off-diagonal to off-diagonal, or off-diagonal to the nearest point on the diagonal (for small values
of $\epsilon$). In this context, distance functions usually applied to evaluate the distance between
two probability density estimations are relevant: the bottleneck distance and the $p$-Wasserstein
distance, where $p\geq 1$. The $p$-Wasserstein distance between two diagrams $D_1$ and $D_2$ is the
infimum over all bijections $\gamma: D_1 \cup \Delta \to D_2 \cup \Delta$, where $\Delta$ is the
multiset $\lbrace (s, s) \mid s \in \mathbb{R} \rbrace$ with multiplicity $(s,s) \mapsto +\infty$,
of
\begin{equation}
  \label{eq:wasserstein_distance}
  \left(\sum_{x \in D_1 \cup \Delta} ||x - \gamma(x)||_q^p \right)^{1/p}
\end{equation}

where we usually have $q=\infty$. When we let $p\to\infty$, we recover the bottleneck distance.

We also use the notion of a median persistence landscape, where, given a collection of PDs, we
compute their associated PL, which is a matrix of fixed dimension $m\times h$ where $m$ is the
length of the vector, and $h$ is the homology dimension. We compute the median PL by taking the
median over all samples for each cell in $m$ for each homology dimension $h$, so we end up with a PL
representative of the collection of the samples in the collection. A similar approach can be adopted
for PIs. The median PL and PI were chosen over the average PL or PI due to the skewed distribution of
the data observed in some cases, as will be highlighted in section \ref{sec:disc-dist}.

Taking the distance between any two persistence landscape or image can be achieved using the
Minkowski distance defined, given two vectors $\mathbf{x},\mathbf{y}\in\mathbb{R}^n$, as:
\begin{equation}
  \label{eq:minkowski}
  D(\mathbf{x},\mathbf{y}) = \left(\sum_{i=1}^{n}|x_i-y_i|^p\right)^{1\over p}
\end{equation}

In this report, we set $p=1$ and henceforth refer to the distance function as the $L^1$ norm.

\subsection{Research questions and outline}

In this report, we address the following research questions:

\begin{enumerate}
\item How salient are the topological features extracted from the patch for the characterization of
  the atrophy observed in Alzheimer's disease as measured through the classification performance of
AD versus CN subjects using persistence images?
\item What does the distance between the persistence image and persistence
landscape of a patient and the median persistence image or landscape of a
diagnostic category reveal about the topological heterogeneity within a given
diagnostic category?
\item How can the distance among persistent images taken for each patient over the course of the
  disease inform us with regard to the progression of the patient during the monitoring period?
\item Does taking the distance between each image and two median persistence
images (from AD patients and CN subjects) allow adequate clustering of patients
in diagnostic categories or disease subtypes?
\end{enumerate}

This report is structured as follows: after having introduced AD and fundamental concepts related to
TDA in this introduction, we go on to present and justify methodological choices we have made
regarding the topological data analysis conducted on sMRI data in section \ref{sec:methods}; in
section \ref{sec:results}, we report the findings extracted from the data, which we discuss in
section \ref{sec:discussion}.

\section{Methods}\label{sec:methods}

Here, we present the methodological choices made for this pipeline. All of the code used to compute
the findings presented in this paper is currently available upon request on
\href{https://github.com/pjhartout/TDA_ADNI_MLCB}{GitHub}.

\subsection{Data}

T1-weighted, 1.5 Tesla sMRIs were obtained from the \href{adni.loni.usc.edu}{Alzheimer's Disease
  Neuroimaging Initiative} (ADNI) database which contains images from AD patients, patients
diagnosed with mild cognitive impairment (MCI) \citep{gauthier2006mild}, and healthy controls
(cognitively normal, CN) of matched age groups \citep{jack2008alzheimer}. Further preprocessing
steps to reduce noise and extract brains structures is highlighted in appendix
\ref{apd:preprocessing}. Then, scans were divided into 216 patches, each of dimension
$30\times36\times30$, providing a possibility for a more focused and computationally efficient
investigation while preserving high resolution. Working with a patch is also supported by the fact
that an investigation of \emph{local} changes in brain architecture may filter out topological
features that are less relevant in the context of Alzheimer's disease.

From earlier work attempting to classify CN subjects from AD patients using a convolutional neural
network (CNN) \citep{bruningk2020image}, we know that a given patch, shown in grey in figure
~\ref{fig:acc}, has a particularly high discriminatory potential, so we selected this patch for all
our further analyses. Support for the use of this patch also comes from its anatomical relevance,
since it contains regions that are most affected in Alzheimer's disease such as the
hippocampus, the entorhinal cortex, and the amygdala ~\citep{goedert2006century}.

\begin{figure}[b]
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/perf_coronal.png}
    \caption{Coronal view of the selected patch.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/perf_saggital.png}
    \caption{Saggital view of the selected patch.}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/perf_ventral.png}
    \caption{Ventral view of the selected patch.}
  \end{subfigure}
  \caption{Accuracy values on each patch. The selected patch is boxed in red.}
  \label{fig:acc}
\end{figure}

\subsection{Topological Data Analysis}\label{sec:tda_setup}

To perform the topological analysis on the patch, we used \texttt{giotto-tda}, a library
specifically made for the integration of TDA pipelines in ML applications ~\citep{tauzin2020giottotda}. Each
filtration on the cubical complexes has been done in three homological dimensions $0,1,2$,
representing features in each of the dimensions of the three-dimensional image. We otherwise used
the default parameters provided by the
\href{https://giotto-ai.github.io/gtda-docs/latest/modules/generated/homology/gtda.homology.CubicalPersistence.html#id2}{giotto-tda
  documentation}. An example persistence diagram for a cognitively intact subject, a patient with MCI, and a
patient with AD are depicted in Figure \ref{fig:sample_rep_pd}. To obtain persistence images for each 3D
volume, we used 0.001 as a standard deviation for the Gaussian kernel, no weight function, and a
default dimension of $100 \times 100$ for each image. Representative samples of these images are
shown in Figure \ref{fig:sample_rep_pi}.

\begin{figure}
  \centering
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PDs/persistence_diagram_CN_H_0.png}
    \caption{PD of a CN patient in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PDs/persistence_diagram_MCI_H_0.png}
    \caption{PD of a MCI patient in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PDs/persistence_diagram_AD_H_0.png}
    \caption{PD of an AD patient in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PDs/persistence_diagram_CN_H_1.png}
    \caption{PD of a CN patient in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PDs/persistence_diagram_MCI_H_1.png}
    \caption{PD of a MCI patient in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PDs/persistence_diagram_AD_H_1.png}
    \caption{PD of an AD patient in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PDs/persistence_diagram_CN_H_2.png}
    \caption{PD of an CN patient in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PDs/persistence_diagram_MCI_H_2.png}
    \caption{PD of an MCI patient in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PDs/persistence_diagram_AD_H_2.png}
    \caption{PD of an AD patient in $H_2$}
  \end{subfigure}
  \caption{Representative PD for each of the diagnostic categories.}
  \label{fig:sample_rep_pd}
\end{figure}


\subsection{Model architecture}\label{sec:model_arch}
For the classification task of classifying AD vs CN patients, we used a parallel
CNN network with one convolutional layer, followed by one dense layer containing 500 neurons and
with dropout rates of 50\% at training time. The output of the last dense layer is redirected to a
single sigmoid neuron for prediction. The model was trained using an exponential decay learning rate
scheduler and early stopping, which monitored the validation loss. All of the layers and utilities
to train the neural network were provided by the Keras library ~\citep{chollet2015keras} and are
available on the \href{https://github.com/pjhartout/TDA_ADNI_MLCB}{repository}, and a depiction of
the computation graph is shown in Figure ~\ref{fig:model_arch}. We also note that the model was
trained on a laptop CPU (Intel(R) Core(TM) i7-9750H CPU @ 2.60GHz). For consistency and to enable a
direct comparison with results shown in \citep{bruningk2020image}, we used the same data
partitioning to present any kind of data leakage that would artificially increase our score, as
discussed in \citep{wen2020convolutional}. We also trained each model three times to mitigate any
performance fluctuations due to different layer initializations.


\begin{figure}
  \centering \includegraphics[width=0.9\textwidth]{figures/model.png}
  \caption{Computation graph to predict the phenotype of a given set of persistent images.}
  \label{fig:model_arch}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/Persistence_image_CN_h_0.png}
    \caption{PI of a CN patient in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/Persistence_image_MCI_h_0.png}
    \caption{PI of a MCI patient in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/Persistence_image_AD_h_0.png}
    \caption{PI of an AD patient in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/Persistence_image_CN_h_1.png}
    \caption{PI of a CN patient in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/Persistence_image_MCI_h_1.png}
    \caption{PI of a MCI patient in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/Persistence_image_AD_h_1.png}
    \caption{PI of an AD patient in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/Persistence_image_CN_h_2.png}
    \caption{PI of an CN patient in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/Persistence_image_MCI_h_2.png}
    \caption{PI of an MCI patient in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/Persistence_image_AD_h_2.png}
    \caption{PI of an AD patient in $H_2$}
  \end{subfigure}
  \caption{Representative PI for each of the diagnostic categories. Each column correponds to a
    diagnostic category whereas each row corresponds to a homological dimension.}
  \label{fig:sample_rep_pi}
\end{figure}

\subsection{Distance between median topological representations}

We also used \texttt{giotto-tda} to compute persistence landscapes. We wanted to keep only the most
prominent features, so we kept only $\lambda_1$, and set the PL vector lengths of 100.

Computing the median persistence landscape for each diagnostic category was done using the median
value of each subject for each of the vector coordinate. The pairwise distance between two PLs was
taken using the $L^1$ norm. Additionally, the Mann-Whitney $\mathcal{U}$ test
was used to test whether two empirical distribution of distances had the same
latent distributions \citep{mann1947test}.

We compute the distances in two settings:
\begin{description}
\item[Intra-diagnostic category setting]: here, we compute the distance of each PL available for all
  available sMRIs belonging to a diagnostic category with respect to the median PL of that category.
\item[Intra-patient distance]: this allows us to assess the distance of the different PDs of the
  same patient, to see if there is any distinctive evolution of the topological features over time.
  In this context, we compute the pairwise distances for all sMRIs from a patient. For some
  analyses, we averaged the pairwise distance for each patient, and grouped patients that changed
  diagnosis over time in one category, and those that did not in another.
\end{description}
We performed each of the analyses on both persistence landscapes and images to highlight differences
among persistent features in the case of the persistence landscapes and all of the topological
features in the case of the persistence image.

\subsection{Relationship between distances and misclassification}

To investigate the relationship between patients departing significantly from the median
representation of the diagnostic category to which they belong and misclassified patients, we chose
to take the $n$ subjects -- hereafter referred to as \emph{topological outliers} -- with the highest
$L_1$ norm from their median representation, and set $n$ to the number of misclassified patients
from one trained network defined in section \ref{sec:model_arch}. We then looked at the overlap
between patients who were misclassified and the selected set of topological outliers, as well as the
distribution of the distance of the misclassified samples compared to the median representation.

\subsection{Distance between multiple median topological representations}\label{sec:methods_dist_multiple_images}

As the last step, we investigated whether the distance of a patient with respect to more than one
median persistence image. We, therefore, evaluated the $L^1$ norm between each persistence image and
both the median AD persistence image and median CN persistence image in $H_2$. We chose
to take these two images because they showed high variability in other analyses (see Figure \ref{fig:displots_median_pi}) and they represent the two
most divergent diagnostic cases in our dataset, MCI being considered as a state
where patients are neither cognitively normal nor formally diagnosed with AD. Given we wanted to maximize
variance across samples for visualization purposes, we first standardized the data and subsequently
applied a principal component analysis to the resulting pairs of distances.

\section{Results}\label{sec:results}

Here we present the results obtained from the above-mentioned pipeline, starting with a performance
assessment of the deep learning model. We then turn our attention to the topological heterogeneity
observed both within each diagnostic category and within each patient. We then present our findings
of the overlap between topological outliers and misclassified samples. Finally, we look at the
distance of each image to the median image representation of AD and CN to see if clusters
emerge.

\subsection{Model Performance}

The performance metrics of the deep learning model is shown in Table ~\ref{tab:performance}, and
seems to be somewhat inferior to state-of-the-art models trained on similar data
\citep{wen2020convolutional}, but requires dramatically less computing power -- the relatively
shallow multilayer CNN shown in \citep{bruningk2020image}, for instance, requires 15 minutes of
training time on a server GPU while our approach requires only 2 minutes on one laptop CPU, showing
the high grade of compression of the approach presented here. Additionally, the model yields higher
results compared to whole-brain persistence images reported in \citep{bruningk2020image}.
Remarkably, we observe that the performance of our network seems to be more stable than other
approaches for which standard deviations of performance metrics are provided.

\begin{table}[b]
  \centering
  \begin{tabular}{lcccc}
    \toprule Local & \textbf{PI} & 3D Conv & & \\ Global &&& PI & SOTA \\ \midrule Validation
    accuracy & $0.79\pm 0.02$ & $0.85\pm 0.06$ & $ 0.76\pm 0.02$ & $0.91$\\ Precision & $0.81\pm
    0.04$ & $0.87\pm0.04$ & $0.74\pm 0.02$& \\ Recall & $0.81\pm 0.02$ & $0.87\pm0.08$ & $0.88\pm
    0.08$ &$0.84$\\ AUC & $0.85\pm 0.03$ & $0.89\pm0.05$ & $ 0.78\pm 0.02$ & $0.96$\\ \bottomrule
    \vspace{1pt}
  \end{tabular}
  \caption{Performance metrics of the local PI approach. Global PI and local 3D Conv approach from
    \citep{bruningk2020image}. SOTA results are obtained from \citep{liu2018anatomical}.}
  \label{tab:performance}
\end{table}

\subsection{Distance analysis}

\subsubsection{$L^1$ norm among diagnostic categories -- topological
  heterogeneity of each diagnostic category}\label{sec:results_between_images}

We now present our findings regarding the distribution of the distances between the PL and PI of
each image with respect to the median PL and PI for each diagnostic category. The representative PL
and PI for each diagnostic category is shown in Figure \ref{fig:median_pls} and
\ref{fig:median_pis}, and the distribution of the $L^1$ norm between each patient and these median
PLs and PIs is shown in Figure \ref{fig:displots_median_pl} and Figure \ref{fig:displots_median_pi},
respectively. As we can see, while the median PLs do not differ too much from one another in each of
the homological dimensions, we see that some persistence images seem to have a far greater distance to this
median PL than the majority of PLs. This is also reflected by the skewness of the distributions
observed in Figure \ref{fig:displots_median_pl} and confirmed in the skewness values shown in Table
\ref{tab:stats_median_pl}. However, the skewness of the distribution does not appear when looking at
the distribution of distance values between PIs, except for AD patients, where skewness
values remain high (Figure \ref{fig:displots_median_pi} and Table \ref{tab:stats_median_pi}).

\begin{figure}
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_CN.png}
    \caption{Median PL for CN subjects}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_MCI.png}
    \caption{Median PL for MCI subjects}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_AD.png}
    \caption{Median PL for AD subjects}
  \end{subfigure}
  \caption{Median persistence landscapes for each of the diagnostic categories.}
  \label{fig:median_pls}
\end{figure}


\begin{figure}
  \centering \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pi_CN_h_0.png}
    \caption{Median CN PI in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pi_MCI_h_0.png}
    \caption{Median MCI PI in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pi_AD_h_0.png}
    \caption{Median AD PI in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pi_CN_h_1.png}
    \caption{Median CN PI in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pi_MCI_h_1.png}
    \caption{Median MCI PI in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pi_AD_h_1.png}
    \caption{Median AD PI in $H_1 $}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pi_CN_h_2.png}
    \caption{Median CN PI in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pi_MCI_h_2.png}
    \caption{Median MCI PI in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pi_AD_h_2.png}
    \caption{Median AD PI in $H_2$}
  \end{subfigure}
  \caption{Median persistence images for each of the diagnostic categories and homological
    dimension.}
  \label{fig:median_pis}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_CN_H_0.png}
    \caption{$L^1$ norm for CN PLs in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_MCI_H_0.png}
    \caption{$L^1$ norm for MCI PLs in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_AD_H_0.png}
    \caption{$L^1$ norm for AD PLs in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_CN_H_1.png}
    \caption{$L^1$ norm for CN PLs in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_MCI_H_1.png}
    \caption{$L^1$ norm for MCI PLs in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_AD_H_1.png}
    \caption{$L^1$ norm for AD PLs in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_CN_H_2.png}
    \caption{$L^1$ norm for CN PLs in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_MCI_H_2.png}
    \caption{$L^1$ norm for MCI PLs in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/median_pls/median_pl_AD_H_2.png}
    \caption{$L^1$ norm for AD PLs in $H_2$}
  \end{subfigure}
  \caption{Histogram showing the distribution of the $L^1$ norm taken between the median PL for a
    diagnostic categories in all homological dimesions.}
  \label{fig:displots_median_pl}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/displot_median_pi_CN_H_0.png}
    \caption{$L^1$ norm for CN PIs in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/displot_median_pi_MCI_H_0.png}
    \caption{$L^1$ norm for MCI PIs in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/displot_median_pi_AD_H_0.png}
    \caption{$L^1$ norm for AD PIs in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/displot_median_pi_CN_H_1.png}
    \caption{$L^1$ norm for CN PIs in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/displot_median_pi_MCI_H_1.png}
    \caption{$L^1$ norm for MCI PIs in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/displot_median_pi_AD_H_1.png}
    \caption{$L^1$ norm for AD PIs in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/displot_median_pi_CN_H_2.png}
    \caption{$L^1$ norm for CN PIs in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/displot_median_pi_MCI_H_2.png}
    \caption{$L^1$ norm for MCI PIs in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/PIs/displot_median_pi_AD_H_2.png}
    \caption{$L^1$ norm for AD PIs in $H_2$}
  \end{subfigure}
  \caption{Histogram showing the distribution of the $L^1$ norm taken between the median PI and each
    image within a diagnostic category in all three homological dimesions.}
  \label{fig:displots_median_pi}
\end{figure}


\begin{table}
\centering
\begin{tabular}{lrrrrrr}
  \toprule {} & Mean & Median & Standard deviation & Q3 & Max & Skewness \\
  \midrule CN $H_0$ & $2.16$ & $2.00$ & $0.78$ & $2.50$ & $7.41$ & $1.78$ \\
  CN $H_1$ & $2.61$ & $2.27$ & $1.17$ & $2.93$ & $9.47$ & $1.92$ \\
  CN $H_2$ & $2.38$ & $2.23$ & $0.88$ & $2.79$ & $7.19$ & $1.39$ \\
  MCI $H_0$ & $2.24$ & $2.04$ & $0.82$ & $2.55$ & $6.21$ & $1.71$ \\
  MCI $H_1$ & $2.57$ & $2.19$ & $1.29$ & $2.80$ & $\mathbf{11.87}$ & $\mathbf{2.57}$ \\
  MCI $H_2$ & $2.40$ & $2.27$ & $0.83$ & $2.82$ & $6.55$ & $1.18$ \\
  AD $H_0$ & $2.40$ & $2.18$ & $0.96$ & $2.77$ & $7.77$ & $1.97$ \\
  AD $H_1$ & $2.47$ & $2.13$ & $1.15$ & $2.77$ & $\mathbf{9.28}$ & $\mathbf{2.10}$ \\
  AD $H_2$ & $2.36$ & $2.20$ & $0.80$ & $2.75$ & $8.39$ & $1.64$ \\
  \bottomrule
  \vspace{1pt}
\end{tabular}
\caption{Summary statistics of the distribution of distances from median persistence landscapes for
each diagnostic category shown in Figure \ref{fig:displots_median_pl}. Highlighted values represent
relatively high values.}
\label{tab:stats_median_pl}
\end{table}


\begin{table}
\centering
\begin{tabular}{lllllll}
  \toprule {}
  & Mean & Median & Standard deviation & Q3 & Max & Skewness \\
  \midrule
  CN $H_0$ & $2.4\times10^{6}$ & $2.3\times10^{6}$ & $2.5\times10^{5}$ & $2.5\times10^{6}$ & $3.2\times10^{6}$ & $0.61$ \\
  CN $H_1$ & $1.1\times10^{7}$ & $1.1\times10^{7}$ & $4.5\times10^{5}$ & $1.1\times10^{7}$ & $1.3\times10^{7}$ & $0.58$ \\
  CN $H_2$ & $9.0\times10^{6}$ & $8.9\times10^{6}$ & $5.5\times10^{5}$ & $9.3\times10^{6}$ & $1.0\times10^{7}$ & $0.55$ \\
  MCI $H_0$ & $2.4\times10^{6}$ & $2.3\times10^{6}$ & $2.4\times10^{5}$ & $2.5\times10^{6}$ & $3.1\times10^{6}$ & $0.35$ \\
  MCI $H_1$ & $1.1\times10^{7}$ & $1.1\times10^{7}$ & $4.5\times10^{5}$ & $1.1\times10^{7}$ & $1.2\times10^{7}$ & $0.4$ \\
  MCI $H_2$ & $9.0\times10^{6}$ & $8.9\times10^{6}$ & $5.4\times10^{5}$ & $9.3\times10^{6}$ & $1.0\times10^{7}$ & $0.3$ \\
  AD $H_0$ & $2.3\times10^{6}$ & $2.3\times10^{6}$ & $2.3\times10^{5}$ & $2.5\times10^{6}$ & $3.6\times10^{6}$ & $0.78$ \\
  AD $H_1$ & $1.1\times10^{7}$ & $1.1\times10^{7}$ & $5.5\times10^{5}$ & $1.1\times10^{7}$ & $1.5\times10^{7}$ & $\mathbf{2.2}$ \\
  AD $H_2$ & $8.4\times10^{6}$ & $8.3\times10^{6}$ & $6.5\times10^{5}$ & $8.8\times10^{6}$ & $1.3\times10^{7}$ & $\mathbf{1.7}$ \\
  \bottomrule
  \vspace{1pt}
\end{tabular}
\caption{Summary statistics of the distribution of distances from median persistence image for each
diagnostic category shown in Figure \ref{fig:displots_median_pi}. Highlighted values represent
relatively high values.}
\label{tab:stats_median_pi}
\end{table}



\subsubsection{Intra-patient pairwise distance and average $L^1$ norm distributions -- topological
  evolution}

As indicated in section \ref{sec:tda_setup}, we can compute the distance between
various PLs associated with the different timepoints available to a single patient
to evaluate the distance between each of these topological representations at
these timepoints, hence obtaining a representation of the topological evolution
of that particular patient. When applying this approach to our data, we find
interesting qualitative results: distances varying widely from one patient to
the next. For instance, if we take a CN patient diagnosed as such throughout the
time that patient has been monitored, we see relatively low distances between
that patients and other timepoints -- see Figure
\ref{fig:patient_evolution_stable} for an example. However, distances seem
higher when taking a patient who transitions from an MCI diagnosis to an AD
diagnosis, as can be seen in Figure ~\ref{fig:patient_evolution_mci_ad}. Note
that the color scale is the same for Figures \ref{fig:patient_evolution_stable}
and \ref{fig:patient_evolution_mci_ad}. However, these effects do not
generalize: if we take the average distance for each homological dimension for
each patient and compare the distribution of these averages for patients who
deteriorate (i.e. transition from CN to MCI or from MCI to AD) and those who do
not, we do not see any quantitative difference, as can be seen in Figure
\ref{fig:kde_intra_patient}. Surprisingly, we see that taking the average
Wasserstein distance for each patient, a bimodal -- similar for both
deteriorating and stable patients -- distribution arises, but mostly disappears as we
let $p\to\infty$ and recover the bottleneck distance. The cause of the distance
change is not known. Additionally, while no substantial differences are
observed, we do see slight rightwards shifts in each mode for both Wasserstein
and the $L^1$ norm of the persistent landscape, indicating a potentially
increased distance distribution among some of the observed data points.
Performing a Mann-Whitney $\mathcal{U}$ test on the data confirms this
qualitative finding, by showing that the two distributions shown in Figure
\ref{fig:kde_intra_patient} stem from two different latent distributions ($p<0.01$ for all distributions).


\begin{figure}
  \centering \includegraphics[width=0.32\textwidth]{figures/temporal_evolution/ADNI011S0023_h_0.png}
  \hfill \includegraphics[width=0.32\textwidth]{figures/temporal_evolution/ADNI011S0023_h_1.png}
  \hfill
  \includegraphics[width=0.32\textwidth]{figures/temporal_evolution/ADNI011S0023_h_2.png} \caption{Topological
    evolution of a subject with an unchanging CN diagnosis.}
  \label{fig:patient_evolution_stable}
\end{figure}

\begin{figure}
  \centering \includegraphics[width=0.32\textwidth]{figures/temporal_evolution/ADNI029S0878_h_0.png}
  \hfill \includegraphics[width=0.32\textwidth]{figures/temporal_evolution/ADNI029S0878_h_1.png}
  \hfill \includegraphics[width=0.32\textwidth]{figures/temporal_evolution/ADNI029S0878_h_2.png}
    \caption{Topological evolution of a patient who transitions from MCI to AD in the course of the
      observation. For this particular case, the change in diagnosis occurred at $t=24$, i.e. 24
      months after the earliest available diagnosis, which also incidentally corresponds with the
highest distance from that baseline.}
    \label{fig:patient_evolution_mci_ad}
\end{figure}


\begin{figure}
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/temporal_evolution/wasserstein_H_0_dist_diag_change.png}
    \caption{Wasserstein distance with $p=2$ in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/temporal_evolution/wasserstein_H_1_dist_diag_change.png}
    \caption{Wasserstein distance with $p=2$ in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/temporal_evolution/wasserstein_H_2_dist_diag_change.png}
    \caption{Wasserstein distance with $p=2$ in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/temporal_evolution/bottleneck_H_0_dist_diag_change.png}
    \caption{Bottleneck distance in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/temporal_evolution/bottleneck_H_1_dist_diag_change.png}
    \caption{Bottleneck distance in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/temporal_evolution/bottleneck_H_2_dist_diag_change.png}
    \caption{Bottleneck distance in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/temporal_evolution/landscape_H_0_dist_diag_change.png}
    \caption{$L^{1}$ landscape distance with 1 layer in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/temporal_evolution/landscape_H_1_dist_diag_change.png}
    \caption{$L^{1}$ landscape distance with 1 layer in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/temporal_evolution/landscape_H_2_dist_diag_change.png}
    \caption{$L^{1}$ landscape distance with 1 layer in $H_2$}
  \end{subfigure}
  \caption{Kernel density estimation of the average average distance between each image timepoint
    for each patient. The orange curve represents all those patients who have had at least one
    change in diagnosis over the course of the disease, whereas patients who have not are within the
    blue curve.}
  \label{fig:kde_intra_patient}
\end{figure}

\subsection{Topological outliers and misclassified samples.}

The distribution of distances with respect to the average persistent landscape was plotted for the
patients who were correctly classified, and for those who were not correctly classified. The results
are shown in Figure~\ref{fig:outlier_misclassified}. We also examined the proportion of patients who
switched diagnoses in the whole ADNI dataset. We found that 70\% (64) of the misclassified patients
had only one diagnosis versus 71\% (323) in the whole dataset, hence indicating that misclassified
patients did not contain persistent features that made them more likely to be misclassified.

\begin{figure}
  \centering
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/misclassification_distance/distribution_distance_misclassified_CN_H_0.png}
    \caption{Distance from median CN PL in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/misclassification_distance/distribution_distance_misclassified_CN_H_1.png}
    \caption{Distance from median CN PL in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/misclassification_distance/distribution_distance_misclassified_CN_H_2.png}
    \caption{Distance from median CN PL in $H_2$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/misclassification_distance/distribution_distance_misclassified_AD_H_0.png}
    \caption{Distance from median AD PL in $H_0$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/misclassification_distance/distribution_distance_misclassified_AD_H_1.png}
    \caption{Distance from median AD PL in $H_1$}
  \end{subfigure}
  \begin{subfigure}{0.32\textwidth}
    \includegraphics[width=\textwidth]{figures/misclassification_distance/distribution_distance_misclassified_AD_H_2.png}
    \caption{Distance from median AD PL in $H_2$}
  \end{subfigure}
    \caption{Kernel density estimation of distribution of the distance between the AD and CN median
      persistence image for images which have not and have been misclassified.}
  \label{fig:outlier_misclassified}
\end{figure}

\subsection{Visualisation of the distance of each patient to the median PI of AD patients and CN patients}

The results of the procedure highlighted in section \ref{sec:methods_dist_multiple_images} is shown
in Figure \ref{fig:vis_pca_ad_cn}. We see that on average AD patients tend to cluster higher up in
the visualization compared to CN subjects. As expected, patients diagnosed with MCI tend to
comingle among AD and CN patients. Clear clusters need yet to be defined more clearly using topological
data analysis.

\begin{figure}
  \centering \includegraphics[width=\textwidth]{figures/cluster_CN_H_2_AD_H_2_PCA.png}
  \caption{Visualisation of the two PCA components obtained from looking at the distance between the
    $L^1$ distance of each image to the median PI of AD and CN in $H_2$.}
  \label{fig:vis_pca_ad_cn}
\end{figure}

\section{Discussion}\label{sec:discussion}

In this section, we begin by discussing how persistence images provide salient features for the
characterization of atrophy due to Alzheimer's disease and result in competitive classification
performance results; we then move on to discuss our findings regarding the distributions of
distances among diagnostic categories and within patients, also touching upon how distances relate
to misclassified samples. Then, we briefly discuss how taking the distance of each image with
respect to two median persistence images yields trends of clusters of patients. Finally, we outline
some limitations and further research avenues to be explored in the future.

\subsection{Local persistence images are salient features for the characterization of atrophy due to AD }

We obtain competitive performance results when classifying AD and CN subjects using persistence
images obtained from patches, showing that the atrophy observed in AD can be characterized
reasonably reliably from local persistence images (shown in Table ~\ref{tab:performance}). While the
classification performance is lower than the state of the art reported in \citep{liu2018anatomical},
which are about 85\% to 90\% and reported in Table \ref{tab:performance}, our results were obtained
using a very simple neural architecture and only the local topological features of a single, small
patch in the temporal lobe. Additionally, the standard deviation of our approach seems to be
consistently lower than other approaches presented so far for which standard deviations of
performance measures were available. This stability is likely because persistence
homology itself is a stable method, robust to noise present in data \citep{cohen2007stability},
but this stability is also compounded by the fact that that persistence images have also been proven
to be stable when introducing noise to the underlying persistence diagram
\cite{adams2017persistence}. We note that one of the inherent impediments to our classifiers is that
it does not have access to all the available clinical data contributing to the establishment of
a formal diagnosis of Alzheimer's disease. As per the revised guidelines for the diagnosis of AD,
other factors such as blood and cerebrospinal fluid biomarkers and positron emission tomography
(PET) scans also greatly influence the establishment of an AD diagnosis \citep{mckhann2011diagnosis}.

Signs showing that our accuracy might be improved is that there is no increased ratio of topological
outliers among the misclassified samples (Figure~\ref{fig:outlier_misclassified}), nor is the
the proportion of patients showing a change in diagnosis substantially higher among misclassified
samples, showing that patients who are oscillating between two diagnostic categories do not account
for a high uncertainty. One way to increase the performance of our models would be a multi-patch
setup, where the persistent image of other relevant patches could be considered. This stems from the
fact that there is increasing evidence for the existence of biological subtypes of AD, which
translate in differentially affected brain regions \citep{poulakis2018heterogeneous,
tijms2020pathophysiological}. In this context, computing the PI of other local areas of the brain
which are affected by other subtypes of AD, like the precuneus, the medial and lateral temporal
cortex, some of which incidentally also show increased accuracy in patch-based classification as
seen in Figure ~\ref{fig:acc}.Another research avenue to be explored is the determination of
possible weight functions applied to obtain persistence images to emphasize particularly important
landmarks of Alzheimer's disease. One might consider combining the results of a gradient-weighted
class activation mapping (Grad-CAM) on a raw CNN trained on the patch with the persistence surface
to increase the weight of particular topological features, should they stand out in the persistence
image and yields better classification results.


\subsection{Distances}\label{sec:disc-dist}

% One interpretation of this results is that, when all the topological % features are taken into account as is the case when computing the % persistence image, and not the most persistent features, as we do when % taking the persistence landscape with one layer, most of the % heterogeneity disappears
% We interpret the skewed distribution of distances among AD patient % images as being the possible result of a preprocessing step where the % brain of the patient is projected onto the MNI space, which might lead % to different results due to the extreme distortion of some brain % regions due to atrophy present in the original image.

As shown in Figure \ref{fig:displots_median_pl}, the distribution of the distances of the persistent
landscapes of each patch PL to the median PL for each of these diagnostic categories (shown in
Figure ~\ref{fig:median_pls}) is very skewed, with some patients' PL having a much higher distance
values compared to the rest of the patients (see Table ~\ref{tab:stats_median_pl} and Figure
~\ref{tab:stats_median_pl}). While the overall skew is most pronounced among MCI patients, pointing
to a genuinely increased topological heterogeneity within this particular diagnostic category, some
of the more extreme values can be attributed to noise introduced at any step of the data acquisition
and preprocessing steps described in section \ref{sec:methods}. Note that this phenomenon could also
underlie the heterogeneity of the results we see in the comparisons made within a single patient
(discussed below), indicating that noise probably plays a significant role in defining the distance
among high persistence features obtained from one-layered persistence landscapes.

The aforementioned skewness and heterogeneity are most likely due to the high diversity of persistent
topological features. As highlighted in section
\ref{sec:theory_persistence_landscape_persistence_image}, the persistence landscape of a persistence
diagrams provides a way to select the most persistent features for a given range of filtration
values and given we have taken the first layer of a persistence landscape in our analyses, the
topological heterogeneity mostly concerns the most persistent features. As noted in section
\ref{sec:results_between_images}, this heterogeneity disappears when performing the same type of
analysis using persistence images as vectorized representations for the analysis. We hypothesize
that this change is because persistence images consider all topological features,
regardless of whether or not they are persistent. Hence, when considering all features, the
distribution is mostly even, and topological outliers cannot be identified, save a few exceptions,
namely AD patients in $H_1$ and $H_2$.

Contrary to expectation, little appreciable difference was seen in intra-patient samples across
distance functions. The reason for this lack of signal is likely because the level of
noise introduced by averaging for each patient likely drown any intra-patient evolution. More
sensitive clustering techniques using PDs could be more useful to determine the temporal trajectory
of each patient. Additionally, the features extracted from a local patch are most likely
not enough to characterize global atrophy progression patterns seen in the cortex of Alzheimer's
patients over time, as noted by \citep{toniolo2018patterns}.

\subsection{Visualizing PIs using distances to the median PI of AD patients and CN subjects}

We now examine Figure \ref{fig:vis_pca_ad_cn}, which plots the principal components obtained from
computing the $L^1$ norm of each image with respect to the median image in $H_2$. We see some trends
emerging: for instance, we tend to see CN PIs cluster in the upper part of the plot while AD PIs
tend to cluster in the bottom, with MCI patients mostly blended in between. Yet, these trends are
not clear enough to obtain clear clusters of disease phenotypes (i.e. one associated with each
condition), let alone disease subtypes. Obtaining features more salient for clustering various
subtypes would probably require more complex features, extracted for instance using a dynamic
autoencoder on the persistence images \citep{mrabah2019deep}. Additionally, other methods tailored
for topological features might also be developed and applied to this analysis pipeline.

\subsection{Limitations and outlook}

The first drawback of our analysis is the difficulty to highlight sources of noise in high
persistence features. For instance, we mention in section \ref{sec:disc-dist} that some of the
topological outliers that were highlighted in Figure \ref{fig:displots_median_pl} (but also observed
among AD patients in Figure \ref{fig:vis_pca_ad_cn}) could be due to noise, but the source of that
noise is unknown. Specifically, it is not possible to investigate whether this noise comes from the
preprocessing pipeline applied to it, or from the latent data distribution. We hypothesize that part
of the noise could be introduced during the mapping of the original T1-weighted image to the
reference normalized MNI space since it is the step that is most likely to introduce artificial
noise in our data except the data itself \citep{collins19943d}, but it is impossible in the
current setup to show evidence that this step in the preprocessing process is the root of the skewed
distribution, or whether these changes can be attributed to intra-individual anatomical variance.

Another general limitation of the findings presented here is the coarseness of the analyses related
to distances. While we wanted to get an insight into the rawest form of the data possible, taking
the $L^1$ norm between some vector representations of the persistence diagram for instance can
artificially drown highly discriminatory features. Therefore, the potential of the topological
features to discriminate between patients who progress from a baseline diagnosis or not can be
further investigated using more optimized clustering techniques making use of the topological
features extracted using persistent homology.

More specifically, some coarseness was deliberately introduced by extracting only one layer when
analyzing persistence landscapes. This value was chosen because we are interested in changes in the
highly persistent features of the data, hence eliminating the noise arising from the persistent
homology computation. On a similar note, the performance of our classifier could have further be
optimized using a deeper and more optimized architecture, but the choice of a simple architecture
was made to assess the saliency of the data rather than the potential of the classifier itself to
yield good results with little computation.

Our classification task does not cover the full spectrum of all of the possible diagnoses a patient
coming to a memory clinic might present. Importantly, our model was not trained to classify patients
who have a case of MCI, which is neither AD nor CN, but in between. The
discriminatory power of the features used in this report do not enable the full-fledged
diagnostic classification task required in the clinic. Further studies need to also assess the
saliency of the PI obtained from the temporal patch in question for a better assessment of the
clinical usefulness of persistent homology in classifying the various categories of patients.

Despite these limitations, it is important to point out that the approach outlined here was
specifically aimed at analyzing MRI images obtained from patients with Alzheimer's disease, it could
also be applied to any other neurodegenerative disorder. Other prevalent neurodegenerative disorders
include Parkinson's disease, dementia with Lewy bodies, and genetically inherited diseases like
Huntington's disease, which all have distinct atrophy patterns and therefore distinct topological signatures.
Adaptations would be required, such as adapting the choice of a patch of interest -- as an example, it
might be more relevant to look at the basal ganglia for Huntington's disease, since the medial
temporal lobe is observed to be mostly spared in Huntington's
\citep{kuhl1982cerebral, halliday1998regional, kassubek2004topography}.

Additionally, we note that a clustering using topological descriptors could be used to more finely
delineate subtypes of AD and various stages of progression. For instance, using deep clustering
techniques and embeddings using persistence images as inputs could yield useful insights into the
various stages of progression and subtypes of dementia. These more sensitive methods could then,
once fine-tuned, hopefully also track preclinical stages of AD, when atrophy is present but does not
result in cognitive decline due to the presence of a cognitive reserve
\citep{scarmeas2004cognitive, van2017neuroimaging}. The identification of patient populations at
risk of developing the disease could then form the target of any potential preventive treatment.

\section{Conclusion}

In this report we have shown that PIs computed from a patch in the temporal lobe are salient for
classifying CN and AD subjects. Additionally, we show that the distribution of distances among the
patients in each of the diagnostic category is skewed, indicating the presence of topological
outliers, but, overall, does not affect the classification performance. Then, we show a significant,
but not substantial, increase in distance among images belonging to a given patient who deteriorates
towards Alzheimer's disease versus patients who do not. Our last finding is that clustering patients
solely according to their distance with respect to multiple median PI does not allow proper AD
subtype identification. Although promising, all of these topology-driven approaches need further
development to maximize the information that can be extracted from MRI images.

\clearpage
\bibliographystyle{unsrtnat} \bibliography{main}

\clearpage
\appendix

\section{Supplements}
\subsection{Preprocessing of MRI data}\label{apd:preprocessing}
We included all T1-weighted MRI images from ADNI 1, 2, 3, and GO, which were captured and
preprocessed by ADNI. Results included in our work come from preprocessing performed using
\texttt{fMRIPrep} 20.1.1, a Nipype 1.5.0 based tool. All MRIs were corrected for intensity
non-uniformity (INU) with \texttt{N4BiasFieldCorrection}, distributed with ANTs 2.2.0, and used as
T1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a
\texttt{Nipype} implementation of the \texttt{antsBrainExtraction.sh} workflow (from ANTs), using
\texttt{OASIS30ANTs} as a target template.
%Brain tissue segmentation of cerebrospinal fluid (CSF),
%white-matter (WM) and gray-matter (GM) was performed on
%the brain-extracted T1w using `fast` [FSL 5.0.7]. %, RRID:SCR_002823, @fsl_fast].
Volume-based spatial normalisation to a standard coordinate space~(\texttt{MNI152NLin2009cAsym}) was
performed through nonlinear registration with \texttt{antsRegistration}, using brain-extracted
versions of both T1w reference and the T1w template. We slected the template `ICMB 152 Nonlinear
Asymmetrical Template Version 2009c' for spatial normalisation.
% [@mni152nlin2009casym, RRID:SCR_008796; TemplateFlow ID: MNI152NLin2009cAsym],
Many internal operations of \texttt{fMRIPrep} use the
\texttt{Nilearn} library, version 0.6.2, % [@nilearn, RRID:SCR_001362]
, mostly within the functional processing workflow.
For more details of the pipeline, please refer to
\href{https://fMRIPrep.readthedocs.io/en/latest/workflows.html}{the official documentation of
  \texttt{fMRIPrep}}. Preprocessing was finalized by intensity normalization of the extracted and
MNI space registered brain images.
%[the section corresponding to workflows in *fMRIPrep*'s documentation](https://fMRIPrep.readthedocs.io/en/latest/workflows.html "fMRIPrep's documentation").

\end{document}
